{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0b68e3c-c8cf-4c84-a99d-b34ad4eca8b1",
   "metadata": {},
   "source": [
    "This is a toy example for using SynaptogenML. \n",
    "---------------------------------------------\n",
    "\n",
    "Please not that in order to have a minimal example, we are using MNIST here. This purpose is to have a usage example only. As stated in out paper, we not believe that it is a good idea to draw any conclusion from experiments with MNIST, especially with a highly simplified network as we are using here.\n",
    "\n",
    "This example should be used as a playground to understand how SynaptogenML can be used ,and which adjustable parameters are available.\n",
    "\n",
    "For proper example setups with SynaptogenML, please have a look at our ASR examples to our publication as listed here: https://github.com/rwth-i6/returnn-experiments/tree/master/2025-memristor-asr\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f9bf0363-0198-4cb0-8f40-f4fc455f9fe0",
   "metadata": {},
   "source": [
    "import copy\n",
    "import sys\n",
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5dd65253-e96b-4554-942f-40910d6fedd8",
   "metadata": {},
   "source": [
    "# please make sure to install the repo itself\n",
    "from synaptogen_ml.quant_modules import LinearQuant, ActivationQuantizer\n",
    "from synaptogen_ml.memristor_modules import TiledMemristorLinear, DacAdcHardwareSettings"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "864e552c-e7db-48bb-b39f-6924f78e34e4",
   "metadata": {},
   "source": [
    "def create_mnist_dataloaders(batch_size):\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    )\n",
    "    print(\"download training data\")\n",
    "    dataset1 = MNIST(\"./\", train=True, download=True, transform=transform)\n",
    "    print(\"download testing data\")\n",
    "    dataset2 = MNIST(\"./\", train=False, transform=transform)\n",
    "    print(\"prepare dataloaders\")\n",
    "    dataloader_train = DataLoader(\n",
    "        dataset=dataset1,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    dataloader_test = DataLoader(\n",
    "        dataset=dataset2,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    return dataloader_train, dataloader_test"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "420dee59-8693-419b-8086-6f910596dfb1",
   "metadata": {},
   "source": [
    "class QuantizedModel(nn.Module):\n",
    "    def __init__(self, weight_precision: int = 4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.weight_precision = weight_precision\n",
    "\n",
    "        self.linear_1 = LinearQuant(\n",
    "            in_features=28 * 28,\n",
    "            out_features=512,\n",
    "            weight_bit_prec=self.weight_precision,\n",
    "            weight_quant_dtype=torch.qint8,\n",
    "            weight_quant_method=\"per_tensor_symmetric\",\n",
    "            bias=False,\n",
    "        )\n",
    "        self.final_linear = LinearQuant(\n",
    "            in_features=512,\n",
    "            out_features=10,\n",
    "            weight_bit_prec=self.weight_precision,\n",
    "            weight_quant_dtype=torch.qint8,\n",
    "            weight_quant_method=\"per_tensor_symmetric\",\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.activation_quant_l1_in = ActivationQuantizer(\n",
    "            bit_precision=8,\n",
    "            dtype=torch.qint8,\n",
    "            method=\"per_tensor_symmetric\",\n",
    "            channel_axis=None,\n",
    "            moving_avrg=None,\n",
    "            reduce_range=False,\n",
    "        )\n",
    "\n",
    "        self.activation_quant_l1_out = ActivationQuantizer(\n",
    "            bit_precision=8,\n",
    "            dtype=torch.qint8,\n",
    "            method=\"per_tensor_symmetric\",\n",
    "            channel_axis=None,\n",
    "            moving_avrg=None,\n",
    "            reduce_range=False,\n",
    "        )\n",
    "\n",
    "        self.activation_quant_final_in = ActivationQuantizer(\n",
    "            bit_precision=8,\n",
    "            dtype=torch.qint8,\n",
    "            method=\"per_tensor_symmetric\",\n",
    "            channel_axis=None,\n",
    "            moving_avrg=None,\n",
    "            reduce_range=False,\n",
    "        )\n",
    "\n",
    "        self.activation_quant_final_out = ActivationQuantizer(\n",
    "            bit_precision=8,\n",
    "            dtype=torch.qint8,\n",
    "            method=\"per_tensor_symmetric\",\n",
    "            channel_axis=None,\n",
    "            moving_avrg=None,\n",
    "            reduce_range=False,\n",
    "        )\n",
    "\n",
    "        self.memristor_linear_1 = None\n",
    "        self.memristor_final = None\n",
    "\n",
    "    def forward(self, image, use_memristor=False):\n",
    "        inp = torch.reshape(image, shape=(-1, 28 * 28))\n",
    "        if use_memristor:\n",
    "            linear_out = self.memristor_linear_1(inp)\n",
    "        else:\n",
    "            linear_out = self.linear_1(self.activation_quant_l1_in(inp))\n",
    "        out1 = nn.functional.tanh(self.activation_quant_l1_out(linear_out))\n",
    "        if use_memristor:\n",
    "            logits = self.memristor_final(out1)\n",
    "        else:\n",
    "            logits = self.final_linear(self.activation_quant_final_in(out1))\n",
    "        quant_out = self.activation_quant_final_out(logits)\n",
    "        return quant_out\n",
    "\n",
    "    def prepare_memristor(self, hardware_settings, mem_array_inputs, mem_array_outputs):\n",
    "        self.memristor_linear_1 = TiledMemristorLinear(\n",
    "            in_features=28 * 28,\n",
    "            out_features=512,\n",
    "            weight_precision=self.weight_precision,\n",
    "            converter_hardware_settings=hardware_settings,\n",
    "            memristor_inputs=mem_array_inputs,\n",
    "            memristor_outputs=mem_array_outputs,\n",
    "        )\n",
    "        self.memristor_final = TiledMemristorLinear(\n",
    "            in_features=512,\n",
    "            out_features=10,\n",
    "            weight_precision=self.weight_precision,\n",
    "            converter_hardware_settings=hardware_settings,\n",
    "            memristor_inputs=mem_array_inputs,\n",
    "            memristor_outputs=mem_array_outputs,\n",
    "        )\n",
    "        self.memristor_linear_1.init_from_linear_quant(\n",
    "            self.activation_quant_l1_in, self.linear_1, num_cycles=0\n",
    "        )\n",
    "        self.memristor_final.init_from_linear_quant(\n",
    "            self.activation_quant_final_in, self.final_linear, num_cycles=0\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "51b8bf8f-4e46-47ce-b8f9-0ec434f544a2",
   "metadata": {},
   "source": [
    "BATCH_SIZE = 10\n",
    "dataloader_train, dataloader_test = create_mnist_dataloaders(BATCH_SIZE)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device: %s\" % device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b5e9de0f-c14d-44e5-b040-7000ba116411",
   "metadata": {},
   "source": [
    "def run_training(\n",
    "    model: nn.Module,\n",
    "    hardware_settings: Optional[DacAdcHardwareSettings],\n",
    "    mem_array_inputs: int = 64,\n",
    "    mem_array_outputs: int = 64,\n",
    "    num_epochs: int = 5,\n",
    "    batch_size: int = 10,\n",
    "    include_memristor_evaluation: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if include_memristor_evaluation:\n",
    "        assert hardware_settings is not None\n",
    "\n",
    "    model.to(device=device)\n",
    "    optimizer = torch.optim.RAdam(lr=1e-4, params=model.parameters())\n",
    "\n",
    "    memristor_accs = []\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        print(\"\\nstart train epoch %i\" % i)\n",
    "        total_ce = 0\n",
    "        total_acc = 0\n",
    "        num_examples = 0\n",
    "        model.to(device=device)\n",
    "        model.train()\n",
    "\n",
    "        for data in dataloader_train:\n",
    "            image, labels = data\n",
    "            num_examples += image.shape[0]\n",
    "            if device == \"cpu\" and num_examples > 2000:\n",
    "                # do not train so much on CPU\n",
    "                break\n",
    "            image = image.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            logits = model.forward(image)\n",
    "            ce = nn.functional.cross_entropy(logits, target=labels, reduction=\"sum\")\n",
    "            total_ce += ce.detach().cpu()\n",
    "            acc = torch.sum(torch.eq(torch.argmax(logits, dim=-1), labels).int())\n",
    "            total_acc += acc.detach().cpu()\n",
    "            ce.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        print(\n",
    "            f\"train ce: {total_ce / num_examples:.3f} acc: {total_acc / num_examples:.3f}\"\n",
    "        )\n",
    "        total_ce = 0\n",
    "        total_acc = 0\n",
    "        num_examples = 0\n",
    "        model.eval()\n",
    "        print(\"\\nstart normal quantized evaluation\")\n",
    "        start = time.time()\n",
    "        for data in dataloader_test:\n",
    "            start_tmp = time.time()\n",
    "            image, labels = data\n",
    "            image = image.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            num_examples += image.shape[0]\n",
    "            with torch.no_grad():\n",
    "                logits = model.forward(image)\n",
    "            ce = nn.functional.cross_entropy(logits, target=labels, reduction=\"sum\")\n",
    "            total_ce += ce.detach().cpu()\n",
    "            acc = torch.sum(torch.eq(torch.argmax(logits, dim=-1), labels).int())\n",
    "            total_acc += acc.detach().cpu()\n",
    "        end_float = time.time() - start\n",
    "        end_float_avg = end_float / num_examples\n",
    "\n",
    "        print(\n",
    "            f\"Normal-quant test ce: {total_ce / num_examples:.6f}, acc: {total_acc / num_examples:.6f}, time: {end_float:.2f}s, per sample: {end_float_avg:.2f}s\"\n",
    "        )\n",
    "\n",
    "        model.prepare_memristor(\n",
    "            hardware_settings=hardware_settings,\n",
    "            mem_array_inputs=mem_array_inputs,\n",
    "            mem_array_outputs=mem_array_outputs,\n",
    "        )\n",
    "        model.to(device=device)\n",
    "\n",
    "        if include_memristor_evaluation:\n",
    "            print(\"\\nstart memristor evaluation\")\n",
    "            start = time.time()\n",
    "            for data in dataloader_test:\n",
    "                start_tmp = time.time()\n",
    "                image, labels = data\n",
    "                image = image.to(device=device)\n",
    "                labels = labels.to(device=device)\n",
    "                num_examples += image.shape[0]\n",
    "                with torch.no_grad():\n",
    "                    logits = model.forward(image, use_memristor=True)\n",
    "                ce = nn.functional.cross_entropy(logits, target=labels, reduction=\"sum\")\n",
    "                total_ce += ce.detach().cpu()\n",
    "                acc = torch.sum(torch.eq(torch.argmax(logits, dim=-1), labels).int())\n",
    "                total_acc += acc.detach().cpu()\n",
    "            end_float = time.time() - start\n",
    "            end_float_avg = end_float / num_examples\n",
    "\n",
    "            memristor_acc = total_acc / num_examples\n",
    "            memristor_accs.append(memristor_acc)\n",
    "            print(\n",
    "                f\"test memristor ce: {total_ce / num_examples:.6f}, acc: {memristor_acc:.6f}, time: {end_float:.2f}s, per sample: {end_float_avg:.2f}s\"\n",
    "            )\n",
    "\n",
    "    assert any(acc >= expected_accuracy for acc in memristor_accs), (\n",
    "        f\"accuracy too low: {max(memristor_accs):.2f} <= {expected_accuracy:.2f}\"\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9a8d03c0-ee15-4c41-8c5a-69b9a5db68f4",
   "metadata": {},
   "source": [
    "hardware_settings = DacAdcHardwareSettings(\n",
    "    input_bits=8,\n",
    "    output_precision_bits=4,\n",
    "    output_range_bits=4,\n",
    "    hardware_input_vmax=0.6,\n",
    "    hardware_output_current_scaling=8020.0,\n",
    ")\n",
    "\n",
    "model = QuantizedModel(weight_precision=4)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0f3c326f-62db-4117-95ef-340133a7c22a",
   "metadata": {},
   "source": [
    "run_training(model=model, hardware_settings=hardware_settings)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "17748c39-f170-4cd2-a165-70d4cd5e46b1",
   "metadata": {},
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
